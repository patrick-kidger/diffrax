{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc5e6de-fd8f-4589-a5bc-8169cdc12ca1",
   "metadata": {},
   "source": [
    "# Neural SDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d198be-5b0f-4d5d-a8d1-a63a3b24ed9d",
   "metadata": {},
   "source": [
    "This example constructs a neural SDE as a generative time series model.\n",
    "\n",
    "An SDE is, of course, random: it defines some distribution. Each sample is a whole path. Thus in modern machine learning parlance, an SDE is a generative time series model. This means it can be trained as a GAN, for example. This does mean we need a discriminator that consumes a path as an input; we use a CDE.\n",
    "\n",
    "Training an SDE as a GAN is precisely what this example does. Doing so will reproduce the following toy example, which is trained on irregularly-sampled time series:\n",
    "\n",
    "![ou](../imgs/neural_sde.png)\n",
    "\n",
    "**References:**\n",
    "\n",
    "Training SDEs as GANs:\n",
    "```bibtex\n",
    "@inproceedings{kidger2021sde1,\n",
    "    title={{N}eural {SDE}s as {I}nfinite-{D}imensional {GAN}s},\n",
    "    author={Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry J},\n",
    "    booktitle = {Proceedings of the 38th International Conference on Machine Learning},\n",
    "    pages = {5453--5463},\n",
    "    year = {2021},\n",
    "    volume = {139},\n",
    "    series = {Proceedings of Machine Learning Research},\n",
    "    publisher = {PMLR},\n",
    "}\n",
    "```\n",
    "\n",
    "Improved training techniques:\n",
    "```bibtex\n",
    "@incollection{kidger2021sde2,\n",
    "    title={{E}fficient and {A}ccurate {G}radients for {N}eural {SDE}s},\n",
    "    author={Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry},\n",
    "    booktitle = {Advances in Neural Information Processing Systems 34},\n",
    "    year = {2021},\n",
    "    publisher = {Curran Associates, Inc.},\n",
    "}\n",
    "```\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/diffrax/blob/main/docs/examples/neural_sde.ipynb).\n",
    "\n",
    "!!! warning\n",
    "\n",
    "    This example will need a GPU to run efficiently.\n",
    "\n",
    "!!! danger \"Advanced example\"\n",
    "\n",
    "    This is an advanced example, due to the complexity of the modelling techniques used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350ecd31-c6f3-4cff-adbc-2f880c40f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88968c8a-7c63-4557-a332-75565b7c8721",
   "metadata": {},
   "source": [
    "LipSwish activation functions are a good choice for the discriminator of an SDE-GAN. (Their use here was introduced in the second reference above.)\n",
    "For simplicity we will actually use LipSwish activations everywhere, even in the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df41f97b-8b00-49c4-84fe-b35f340b7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lipswish(x):\n",
    "    return 0.909 * jnn.silu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415f14e-9195-4179-9430-0983faecab39",
   "metadata": {},
   "source": [
    "Now set up the vector fields appearing on the right hand side of each differential equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592dad43-7a89-4485-8b74-7855931d2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorField(eqx.Module):\n",
    "    scale: int | jnp.ndarray\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, hidden_size, width_size, depth, scale, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        scale_key, mlp_key = jr.split(key)\n",
    "        if scale:\n",
    "            self.scale = jr.uniform(scale_key, (hidden_size,), minval=0.9, maxval=1.1)\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        t = jnp.asarray(t)\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y]))\n",
    "\n",
    "\n",
    "class ControlledVectorField(eqx.Module):\n",
    "    scale: int | jnp.ndarray\n",
    "    mlp: eqx.nn.MLP\n",
    "    control_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self, control_size, hidden_size, width_size, depth, scale, *, key, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        scale_key, mlp_key = jr.split(key)\n",
    "        if scale:\n",
    "            self.scale = jr.uniform(\n",
    "                scale_key, (hidden_size, control_size), minval=0.9, maxval=1.1\n",
    "            )\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size * control_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "        self.control_size = control_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        t = jnp.asarray(t)\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y])).reshape(\n",
    "            self.hidden_size, self.control_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4633fb9-97af-4e22-b903-40a1fc6c92ad",
   "metadata": {},
   "source": [
    "Now set up the neural SDE (the generator) and the neural CDE (the discriminator).\n",
    "\n",
    "- Note the use of very large step sizes. By using a large step size we essentially \"bake in\" the discretisation. This is quite a standard thing to do to decrease computational costs, when the vector field is a pure neural network. (You can reduce the step size here if you want to -- which will increase the computational cost, of course.)\n",
    "\n",
    "- Note the `clip_weights` method on the CDE -- this is part of imposing the Lipschitz condition on the discriminator of a Wasserstein GAN.\n",
    "(The other thing doing this is the use of those LipSwish activation functions we saw earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c157fe-4c86-4e15-9020-b523b517ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    vf: VectorField  # drift\n",
    "    cvf: ControlledVectorField  # diffusion\n",
    "    readout: eqx.nn.Linear\n",
    "    initial_noise_size: int\n",
    "    noise_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        *,\n",
    "        key,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        initial_key, vf_key, cvf_key, readout_key = jr.split(key, 4)\n",
    "\n",
    "        self.initial = eqx.nn.MLP(\n",
    "            initial_noise_size, hidden_size, width_size, depth, key=initial_key\n",
    "        )\n",
    "        self.vf = VectorField(hidden_size, width_size, depth, scale=True, key=vf_key)\n",
    "        self.cvf = ControlledVectorField(\n",
    "            noise_size, hidden_size, width_size, depth, scale=True, key=cvf_key\n",
    "        )\n",
    "        self.readout = eqx.nn.Linear(hidden_size, data_size, key=readout_key)\n",
    "\n",
    "        self.initial_noise_size = initial_noise_size\n",
    "        self.noise_size = noise_size\n",
    "\n",
    "    def __call__(self, ts, *, key):\n",
    "        t0 = ts[0]\n",
    "        t1 = ts[-1]\n",
    "        # Very large dt0 for computational speed\n",
    "        dt0 = 1.0\n",
    "        init_key, bm_key = jr.split(key, 2)\n",
    "        init = jr.normal(init_key, (self.initial_noise_size,))\n",
    "        control = diffrax.VirtualBrownianTree(\n",
    "            t0=t0, t1=t1, tol=dt0 / 2, shape=(self.noise_size,), key=bm_key\n",
    "        )\n",
    "        vf = diffrax.ODETerm(self.vf)  # Drift term\n",
    "        cvf = diffrax.ControlTerm(self.cvf, control)  # Diffusion term\n",
    "        terms = diffrax.MultiTerm(vf, cvf)\n",
    "        # ReversibleHeun is a cheap choice of SDE solver. We could also use Euler etc.\n",
    "        solver = diffrax.ReversibleHeun()\n",
    "        y0 = self.initial(init)\n",
    "        saveat = diffrax.SaveAt(ts=ts)\n",
    "        sol = diffrax.diffeqsolve(terms, solver, t0, t1, dt0, y0, saveat=saveat)\n",
    "        return jax.vmap(self.readout)(sol.ys)\n",
    "\n",
    "\n",
    "class NeuralCDE(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    vf: VectorField\n",
    "    cvf: ControlledVectorField\n",
    "    readout: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        initial_key, vf_key, cvf_key, readout_key = jr.split(key, 4)\n",
    "\n",
    "        self.initial = eqx.nn.MLP(\n",
    "            data_size + 1, hidden_size, width_size, depth, key=initial_key\n",
    "        )\n",
    "        self.vf = VectorField(hidden_size, width_size, depth, scale=False, key=vf_key)\n",
    "        self.cvf = ControlledVectorField(\n",
    "            data_size, hidden_size, width_size, depth, scale=False, key=cvf_key\n",
    "        )\n",
    "        self.readout = eqx.nn.Linear(hidden_size, 1, key=readout_key)\n",
    "\n",
    "    def __call__(self, ts, ys):\n",
    "        # Interpolate data into a continuous path.\n",
    "        ys = diffrax.linear_interpolation(\n",
    "            ts, ys, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "        init = jnp.concatenate([ts[0, None], ys[0]])\n",
    "        control = diffrax.LinearInterpolation(ts, ys)\n",
    "        vf = diffrax.ODETerm(self.vf)\n",
    "        cvf = diffrax.ControlTerm(self.cvf, control)\n",
    "        terms = diffrax.MultiTerm(vf, cvf)\n",
    "        solver = diffrax.ReversibleHeun()\n",
    "        t0 = ts[0]\n",
    "        t1 = ts[-1]\n",
    "        dt0 = 1.0\n",
    "        y0 = self.initial(init)\n",
    "        # Have the discriminator produce an output at both `t0` *and* `t1`.\n",
    "        # The output at `t0` has only seen the initial point of a sample. This gives\n",
    "        # additional supervision to the distribution learnt for the initial condition.\n",
    "        # The output at `t1` has seen the entire path of a sample. This is needed to\n",
    "        # actually learn the evolving trajectory.\n",
    "        saveat = diffrax.SaveAt(t0=True, t1=True)\n",
    "        sol = diffrax.diffeqsolve(terms, solver, t0, t1, dt0, y0, saveat=saveat)\n",
    "        return jax.vmap(self.readout)(sol.ys)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def clip_weights(self):\n",
    "        leaves, treedef = jax.tree_util.tree_flatten(\n",
    "            self, is_leaf=lambda x: isinstance(x, eqx.nn.Linear)\n",
    "        )\n",
    "        new_leaves = []\n",
    "        for leaf in leaves:\n",
    "            if isinstance(leaf, eqx.nn.Linear):\n",
    "                lim = 1 / leaf.out_features\n",
    "                leaf = eqx.tree_at(\n",
    "                    lambda x: x.weight, leaf, leaf.weight.clip(-lim, lim)\n",
    "                )\n",
    "            new_leaves.append(leaf)\n",
    "        return jax.tree_util.tree_unflatten(treedef, new_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bc8f0-2309-43ea-a23f-7e7f558e9691",
   "metadata": {},
   "source": [
    "Next, the dataset. This follows the trajectories you can see in the picture above. (Namely positive drift with mean-reversion and time-dependent diffusion.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a181d457-2ff5-4eac-8943-ca9e83faeb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "@jax.vmap\n",
    "def get_data(key):\n",
    "    bm_key, y0_key, drop_key = jr.split(key, 3)\n",
    "\n",
    "    mu = 0.02\n",
    "    theta = 0.1\n",
    "    sigma = 0.4\n",
    "\n",
    "    t0 = 0\n",
    "    t1 = 63\n",
    "    t_size = 64\n",
    "\n",
    "    def drift(t, y, args):\n",
    "        return mu * t - theta * y\n",
    "\n",
    "    def diffusion(t, y, args):\n",
    "        return jnp.array([2 * sigma * t / t1])\n",
    "\n",
    "    bm = diffrax.UnsafeBrownianPath(shape=(), key=bm_key)\n",
    "    drift = diffrax.ODETerm(drift)\n",
    "    diffusion = diffrax.ControlTerm(diffusion, bm)\n",
    "    terms = diffrax.MultiTerm(drift, diffusion)\n",
    "    solver = diffrax.Euler()\n",
    "    dt0 = 0.1\n",
    "    y0 = jr.uniform(y0_key, (1,), minval=-1, maxval=1)\n",
    "    ts = jnp.linspace(t0, t1, t_size)\n",
    "    saveat = diffrax.SaveAt(ts=ts)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        terms, solver, t0, t1, dt0, y0, saveat=saveat, adjoint=diffrax.DirectAdjoint()\n",
    "    )\n",
    "\n",
    "    # Make the data irregularly sampled\n",
    "    to_drop = jr.bernoulli(drop_key, 0.3, (t_size, 1))\n",
    "    ys = jnp.where(to_drop, jnp.nan, sol.ys)\n",
    "\n",
    "    return ts, ys\n",
    "\n",
    "\n",
    "def dataloader(arrays, batch_size, loop, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jr.permutation(key, indices)\n",
    "        key = jr.split(key, 1)[0]\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "        if not loop:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d48135-e8ff-411c-aa1c-2c3b6d55b4c4",
   "metadata": {},
   "source": [
    "Now the usual training step for GAN training.\n",
    "\n",
    "There is one neural-SDE-specific trick here: we increase the update size (i.e. the learning rate) for those parameters describing (and discriminating) the initial condition of the SDE. Otherwise the model tends to focus just on fitting just the rest of the data (i.e. the random evolution over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ec8e37-1aaa-4623-9601-9b21175708eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def loss(generator, discriminator, ts_i, ys_i, key, step=0):\n",
    "    batch_size, _ = ts_i.shape\n",
    "    key = jr.fold_in(key, step)\n",
    "    key = jr.split(key, batch_size)\n",
    "    fake_ys_i = jax.vmap(generator)(ts_i, key=key)\n",
    "    real_score = jax.vmap(discriminator)(ts_i, ys_i)\n",
    "    fake_score = jax.vmap(discriminator)(ts_i, fake_ys_i)\n",
    "    return jnp.mean(real_score - fake_score)\n",
    "\n",
    "\n",
    "@eqx.filter_grad\n",
    "def grad_loss(g_d, ts_i, ys_i, key, step):\n",
    "    generator, discriminator = g_d\n",
    "    return loss(generator, discriminator, ts_i, ys_i, key, step)\n",
    "\n",
    "\n",
    "def increase_update_initial(updates):\n",
    "    get_initial_leaves = lambda u: jax.tree_util.tree_leaves(u.initial)\n",
    "    return eqx.tree_at(get_initial_leaves, updates, replace_fn=lambda x: x * 10)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    g_opt_state,\n",
    "    d_opt_state,\n",
    "    g_optim,\n",
    "    d_optim,\n",
    "    ts_i,\n",
    "    ys_i,\n",
    "    key,\n",
    "    step,\n",
    "):\n",
    "    g_grad, d_grad = grad_loss((generator, discriminator), ts_i, ys_i, key, step)\n",
    "    g_updates, g_opt_state = g_optim.update(g_grad, g_opt_state)\n",
    "    d_updates, d_opt_state = d_optim.update(d_grad, d_opt_state)\n",
    "    g_updates = increase_update_initial(g_updates)\n",
    "    d_updates = increase_update_initial(d_updates)\n",
    "    generator = eqx.apply_updates(generator, g_updates)\n",
    "    discriminator = eqx.apply_updates(discriminator, d_updates)\n",
    "    discriminator = discriminator.clip_weights()\n",
    "    return generator, discriminator, g_opt_state, d_opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421f671-dcc6-4338-a8f7-9fd17d6aee37",
   "metadata": {},
   "source": [
    "This is our main entry point. Try running `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0581722-97fb-4771-94da-c65f9929e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    initial_noise_size=5,\n",
    "    noise_size=3,\n",
    "    hidden_size=16,\n",
    "    width_size=16,\n",
    "    depth=1,\n",
    "    generator_lr=2e-5,\n",
    "    discriminator_lr=1e-4,\n",
    "    batch_size=1024,\n",
    "    steps=10000,\n",
    "    steps_per_print=200,\n",
    "    dataset_size=8192,\n",
    "    seed=5678,\n",
    "):\n",
    "    key = jr.PRNGKey(seed)\n",
    "    (\n",
    "        data_key,\n",
    "        generator_key,\n",
    "        discriminator_key,\n",
    "        dataloader_key,\n",
    "        train_key,\n",
    "        evaluate_key,\n",
    "        sample_key,\n",
    "    ) = jr.split(key, 7)\n",
    "    data_key = jr.split(data_key, dataset_size)\n",
    "\n",
    "    ts, ys = get_data(data_key)\n",
    "    _, _, data_size = ys.shape\n",
    "\n",
    "    generator = NeuralSDE(\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        key=generator_key,\n",
    "    )\n",
    "    discriminator = NeuralCDE(\n",
    "        data_size, hidden_size, width_size, depth, key=discriminator_key\n",
    "    )\n",
    "\n",
    "    g_optim = optax.rmsprop(generator_lr)\n",
    "    d_optim = optax.rmsprop(-discriminator_lr)\n",
    "    g_opt_state = g_optim.init(eqx.filter(generator, eqx.is_inexact_array))\n",
    "    d_opt_state = d_optim.init(eqx.filter(discriminator, eqx.is_inexact_array))\n",
    "\n",
    "    infinite_dataloader = dataloader(\n",
    "        (ts, ys), batch_size, loop=True, key=dataloader_key\n",
    "    )\n",
    "\n",
    "    for step, (ts_i, ys_i) in zip(range(steps), infinite_dataloader):\n",
    "        step = jnp.asarray(step)\n",
    "        generator, discriminator, g_opt_state, d_opt_state = make_step(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            g_opt_state,\n",
    "            d_opt_state,\n",
    "            g_optim,\n",
    "            d_optim,\n",
    "            ts_i,\n",
    "            ys_i,\n",
    "            key,\n",
    "            step,\n",
    "        )\n",
    "        if (step % steps_per_print) == 0 or step == steps - 1:\n",
    "            total_score = 0\n",
    "            num_batches = 0\n",
    "            for ts_i, ys_i in dataloader(\n",
    "                (ts, ys), batch_size, loop=False, key=evaluate_key\n",
    "            ):\n",
    "                score = loss(generator, discriminator, ts_i, ys_i, sample_key)\n",
    "                total_score += score.item()\n",
    "                num_batches += 1\n",
    "            print(f\"Step: {step}, Loss: {total_score / num_batches}\")\n",
    "\n",
    "    # Plot samples\n",
    "    fig, ax = plt.subplots()\n",
    "    num_samples = min(50, dataset_size)\n",
    "    ts_to_plot = ts[:num_samples]\n",
    "    ys_to_plot = ys[:num_samples]\n",
    "\n",
    "    def _interp(ti, yi):\n",
    "        return diffrax.linear_interpolation(\n",
    "            ti, yi, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "\n",
    "    ys_to_plot = jax.vmap(_interp)(ts_to_plot, ys_to_plot)[..., 0]\n",
    "    ys_sampled = jax.vmap(generator)(ts_to_plot, key=jr.split(sample_key, num_samples))[\n",
    "        ..., 0\n",
    "    ]\n",
    "    kwargs = dict(label=\"Real\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_to_plot):\n",
    "        ax.plot(ti, yi, c=\"dodgerblue\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    kwargs = dict(label=\"Generated\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_sampled):\n",
    "        ax.plot(ti, yi, c=\"crimson\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    ax.set_title(f\"{num_samples} samples from both real and generated distributions.\")\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"neural_sde.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f182fe77-e4d2-4094-88c5-926cf2b1f8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TracerArrayConversionError",
     "evalue": "The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nThe error occurred while tracing the function _fn at /Users/owenlockwood/miniforge3/envs/dev_diffrax/lib/python3.10/site-packages/equinox/_eval_shape.py:31 for jit. This concrete value was not available in Python because it depends on the values of the arguments _dynamic[1][0].tprev and _dynamic[1][0].tnext.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/dev_diffrax/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3209\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3209\u001b[0m     \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39;49mndim\n\u001b[1;32m   3210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[7], line 54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(initial_noise_size, noise_size, hidden_size, width_size, depth, generator_lr, discriminator_lr, batch_size, steps, steps_per_print, dataset_size, seed)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m step, (ts_i, ys_i) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mrange\u001b[39m(steps), infinite_dataloader):\n\u001b[1;32m     53\u001b[0m     step \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39masarray(step)\n\u001b[0;32m---> 54\u001b[0m     generator, discriminator, g_opt_state, d_opt_state \u001b[39m=\u001b[39m make_step(\n\u001b[1;32m     55\u001b[0m         generator,\n\u001b[1;32m     56\u001b[0m         discriminator,\n\u001b[1;32m     57\u001b[0m         g_opt_state,\n\u001b[1;32m     58\u001b[0m         d_opt_state,\n\u001b[1;32m     59\u001b[0m         g_optim,\n\u001b[1;32m     60\u001b[0m         d_optim,\n\u001b[1;32m     61\u001b[0m         ts_i,\n\u001b[1;32m     62\u001b[0m         ys_i,\n\u001b[1;32m     63\u001b[0m         key,\n\u001b[1;32m     64\u001b[0m         step,\n\u001b[1;32m     65\u001b[0m     )\n\u001b[1;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m (step \u001b[39m%\u001b[39m steps_per_print) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m step \u001b[39m==\u001b[39m steps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     67\u001b[0m         total_score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mmake_step\u001b[0;34m(generator, discriminator, g_opt_state, d_opt_state, g_optim, d_optim, ts_i, ys_i, key, step)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m@eqx\u001b[39m\u001b[39m.\u001b[39mfilter_jit\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_step\u001b[39m(\n\u001b[1;32m     25\u001b[0m     generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     step,\n\u001b[1;32m     35\u001b[0m ):\n\u001b[0;32m---> 36\u001b[0m     g_grad, d_grad \u001b[39m=\u001b[39m grad_loss((generator, discriminator), ts_i, ys_i, key, step)\n\u001b[1;32m     37\u001b[0m     g_updates, g_opt_state \u001b[39m=\u001b[39m g_optim\u001b[39m.\u001b[39mupdate(g_grad, g_opt_state)\n\u001b[1;32m     38\u001b[0m     d_updates, d_opt_state \u001b[39m=\u001b[39m d_optim\u001b[39m.\u001b[39mupdate(d_grad, d_opt_state)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m, in \u001b[0;36mgrad_loss\u001b[0;34m(g_d, ts_i, ys_i, key, step)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39m@eqx\u001b[39m\u001b[39m.\u001b[39mfilter_grad\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad_loss\u001b[39m(g_d, ts_i, ys_i, key, step):\n\u001b[1;32m     14\u001b[0m     generator, discriminator \u001b[39m=\u001b[39m g_d\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m loss(generator, discriminator, ts_i, ys_i, key, step)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(generator, discriminator, ts_i, ys_i, key, step)\u001b[0m\n\u001b[1;32m      4\u001b[0m key \u001b[39m=\u001b[39m jr\u001b[39m.\u001b[39mfold_in(key, step)\n\u001b[1;32m      5\u001b[0m key \u001b[39m=\u001b[39m jr\u001b[39m.\u001b[39msplit(key, batch_size)\n\u001b[0;32m----> 6\u001b[0m fake_ys_i \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mvmap(generator)(ts_i, key\u001b[39m=\u001b[39;49mkey)\n\u001b[1;32m      7\u001b[0m real_score \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(discriminator)(ts_i, ys_i)\n\u001b[1;32m      8\u001b[0m fake_score \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(discriminator)(ts_i, fake_ys_i)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m, in \u001b[0;36mNeuralSDE.__call__\u001b[0;34m(self, ts, key)\u001b[0m\n\u001b[1;32m     51\u001b[0m y0 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitial(init)\n\u001b[1;32m     52\u001b[0m saveat \u001b[39m=\u001b[39m diffrax\u001b[39m.\u001b[39mSaveAt(ts\u001b[39m=\u001b[39mts)\n\u001b[0;32m---> 53\u001b[0m sol \u001b[39m=\u001b[39m diffrax\u001b[39m.\u001b[39;49mdiffeqsolve(terms, solver, t0, t1, dt0, y0, saveat\u001b[39m=\u001b[39;49msaveat)\n\u001b[1;32m     54\u001b[0m \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39mvmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreadout)(sol\u001b[39m.\u001b[39mys)\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_integrate.py:1464\u001b[0m, in \u001b[0;36mdiffeqsolve\u001b[0;34m(terms, solver, t0, t1, dt0, y0, args, saveat, stepsize_controller, adjoint, event, max_steps, throw, progress_meter, solver_state, controller_state, made_jump, path_state, discrete_terminating_event)\u001b[0m\n\u001b[1;32m   1436\u001b[0m init_state \u001b[39m=\u001b[39m State(\n\u001b[1;32m   1437\u001b[0m     y\u001b[39m=\u001b[39my0,\n\u001b[1;32m   1438\u001b[0m     tprev\u001b[39m=\u001b[39mtprev,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     event_mask\u001b[39m=\u001b[39mevent_mask,\n\u001b[1;32m   1458\u001b[0m )\n\u001b[1;32m   1460\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \u001b[39m# Main loop\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m-> 1464\u001b[0m final_state, aux_stats \u001b[39m=\u001b[39m adjoint\u001b[39m.\u001b[39;49mloop(\n\u001b[1;32m   1465\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1466\u001b[0m     terms\u001b[39m=\u001b[39;49mterms,\n\u001b[1;32m   1467\u001b[0m     solver\u001b[39m=\u001b[39;49msolver,\n\u001b[1;32m   1468\u001b[0m     stepsize_controller\u001b[39m=\u001b[39;49mstepsize_controller,\n\u001b[1;32m   1469\u001b[0m     event\u001b[39m=\u001b[39;49mevent,\n\u001b[1;32m   1470\u001b[0m     saveat\u001b[39m=\u001b[39;49msaveat,\n\u001b[1;32m   1471\u001b[0m     t0\u001b[39m=\u001b[39;49mt0,\n\u001b[1;32m   1472\u001b[0m     t1\u001b[39m=\u001b[39;49mt1,\n\u001b[1;32m   1473\u001b[0m     dt0\u001b[39m=\u001b[39;49mdt0,\n\u001b[1;32m   1474\u001b[0m     max_steps\u001b[39m=\u001b[39;49mmax_steps,\n\u001b[1;32m   1475\u001b[0m     init_state\u001b[39m=\u001b[39;49minit_state,\n\u001b[1;32m   1476\u001b[0m     throw\u001b[39m=\u001b[39;49mthrow,\n\u001b[1;32m   1477\u001b[0m     passed_solver_state\u001b[39m=\u001b[39;49mpassed_solver_state,\n\u001b[1;32m   1478\u001b[0m     passed_controller_state\u001b[39m=\u001b[39;49mpassed_controller_state,\n\u001b[1;32m   1479\u001b[0m     passed_path_state\u001b[39m=\u001b[39;49mpassed_path_state,\n\u001b[1;32m   1480\u001b[0m     progress_meter\u001b[39m=\u001b[39;49mprogress_meter,\n\u001b[1;32m   1481\u001b[0m )\n\u001b[1;32m   1483\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[39m# Finish up\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m progress_meter\u001b[39m.\u001b[39mclose(final_state\u001b[39m.\u001b[39mprogress_meter_state)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_adjoint.py:308\u001b[0m, in \u001b[0;36mRecursiveCheckpointAdjoint.loop\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    304\u001b[0m     outer_while_loop \u001b[39m=\u001b[39m ft\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    305\u001b[0m         _outer_loop, kind\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcheckpointed\u001b[39m\u001b[39m\"\u001b[39m, checkpoints\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoints\n\u001b[1;32m    306\u001b[0m     )\n\u001b[1;32m    307\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m final_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_loop(\n\u001b[1;32m    309\u001b[0m     terms\u001b[39m=\u001b[39;49mterms,\n\u001b[1;32m    310\u001b[0m     saveat\u001b[39m=\u001b[39;49msaveat,\n\u001b[1;32m    311\u001b[0m     init_state\u001b[39m=\u001b[39;49minit_state,\n\u001b[1;32m    312\u001b[0m     max_steps\u001b[39m=\u001b[39;49mmax_steps,\n\u001b[1;32m    313\u001b[0m     inner_while_loop\u001b[39m=\u001b[39;49minner_while_loop,\n\u001b[1;32m    314\u001b[0m     outer_while_loop\u001b[39m=\u001b[39;49mouter_while_loop,\n\u001b[1;32m    315\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    316\u001b[0m )\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m msg \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     final_state \u001b[39m=\u001b[39m eqxi\u001b[39m.\u001b[39mnondifferentiable_backward(\n\u001b[1;32m    319\u001b[0m         final_state, msg\u001b[39m=\u001b[39mmsg, symbolic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_integrate.py:624\u001b[0m, in \u001b[0;36mloop\u001b[0;34m(solver, stepsize_controller, event, saveat, t0, t1, dt0, max_steps, terms, args, init_state, inner_while_loop, outer_while_loop, progress_meter)\u001b[0m\n\u001b[1;32m    622\u001b[0m static_made_jump \u001b[39m=\u001b[39m init_state\u001b[39m.\u001b[39mmade_jump\n\u001b[1;32m    623\u001b[0m static_result \u001b[39m=\u001b[39m init_state\u001b[39m.\u001b[39mresult\n\u001b[0;32m--> 624\u001b[0m _, traced_jump, traced_result \u001b[39m=\u001b[39m eqx\u001b[39m.\u001b[39;49mfilter_eval_shape(body_fun_aux, init_state)\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m traced_jump:\n\u001b[1;32m    626\u001b[0m     static_made_jump \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_integrate.py:351\u001b[0m, in \u001b[0;36mloop.<locals>.body_fun_aux\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    344\u001b[0m state \u001b[39m=\u001b[39m _handle_static(state)\n\u001b[1;32m    346\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m# Actually do some differential equation solving! Make numerical steps, adapt\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m# step sizes, all that jazz.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m (y, y_error, dense_info, solver_state, path_state, solver_result) \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    352\u001b[0m     terms,\n\u001b[1;32m    353\u001b[0m     state\u001b[39m.\u001b[39;49mtprev,\n\u001b[1;32m    354\u001b[0m     state\u001b[39m.\u001b[39;49mtnext,\n\u001b[1;32m    355\u001b[0m     state\u001b[39m.\u001b[39;49my,\n\u001b[1;32m    356\u001b[0m     args,\n\u001b[1;32m    357\u001b[0m     state\u001b[39m.\u001b[39;49msolver_state,\n\u001b[1;32m    358\u001b[0m     state\u001b[39m.\u001b[39;49mmade_jump,\n\u001b[1;32m    359\u001b[0m     state\u001b[39m.\u001b[39;49mpath_state,\n\u001b[1;32m    360\u001b[0m )\n\u001b[1;32m    362\u001b[0m \u001b[39m# e.g. if someone has a sqrt(y) in the vector field, and dt0 is so large that\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m# we get a negative value for y, and then get a NaN vector field. (And then\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m# everything breaks.) See #143.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m y_error \u001b[39m=\u001b[39m jtu\u001b[39m.\u001b[39mtree_map(\u001b[39mlambda\u001b[39;00m x: jnp\u001b[39m.\u001b[39mwhere(jnp\u001b[39m.\u001b[39misnan(x), jnp\u001b[39m.\u001b[39minf, x), y_error)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_solver/reversible_heun.py:80\u001b[0m, in \u001b[0;36mReversibleHeun.step\u001b[0;34m(self, terms, t0, t1, y0, args, solver_state, made_jump, path_state)\u001b[0m\n\u001b[1;32m     77\u001b[0m vf0 \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mcond(made_jump, \u001b[39mlambda\u001b[39;00m _: terms\u001b[39m.\u001b[39mvf(t0, y0, args), \u001b[39mlambda\u001b[39;00m _: vf0, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m control, new_path_state \u001b[39m=\u001b[39m terms\u001b[39m.\u001b[39mcontr(t0, t1, path_state)\n\u001b[0;32m---> 80\u001b[0m yhat1 \u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m y0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mω \u001b[39m-\u001b[39m yhat0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mω \u001b[39m+\u001b[39m terms\u001b[39m.\u001b[39;49mprod(vf0, control) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m ω)\u001b[39m.\u001b[39mω\n\u001b[1;32m     81\u001b[0m vf1 \u001b[39m=\u001b[39m terms\u001b[39m.\u001b[39mvf(t1, yhat1, args)\n\u001b[1;32m     82\u001b[0m y1 \u001b[39m=\u001b[39m (y0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mω \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m terms\u001b[39m.\u001b[39mprod((vf0\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mω \u001b[39m+\u001b[39m vf1\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mω)\u001b[39m.\u001b[39mω, control) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m ω)\u001b[39m.\u001b[39mω\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_term.py:614\u001b[0m, in \u001b[0;36mMultiTerm.prod\u001b[0;34m(self, vf, control)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprod\u001b[39m(\n\u001b[1;32m    612\u001b[0m     \u001b[39mself\u001b[39m, vf: \u001b[39mtuple\u001b[39m[PyTree[ArrayLike], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], control: \u001b[39mtuple\u001b[39m[PyTree[ArrayLike], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[1;32m    613\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Y:\n\u001b[0;32m--> 614\u001b[0m     out \u001b[39m=\u001b[39m [\n\u001b[1;32m    615\u001b[0m         term\u001b[39m.\u001b[39mprod(vf_, control_)\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m term, vf_, control_ \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterms, vf, control)\n\u001b[1;32m    617\u001b[0m     ]\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m jtu\u001b[39m.\u001b[39mtree_map(_sum, \u001b[39m*\u001b[39mout)\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_term.py:615\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprod\u001b[39m(\n\u001b[1;32m    612\u001b[0m     \u001b[39mself\u001b[39m, vf: \u001b[39mtuple\u001b[39m[PyTree[ArrayLike], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], control: \u001b[39mtuple\u001b[39m[PyTree[ArrayLike], \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n\u001b[1;32m    613\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Y:\n\u001b[1;32m    614\u001b[0m     out \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 615\u001b[0m         term\u001b[39m.\u001b[39;49mprod(vf_, control_)\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m term, vf_, control_ \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterms, vf, control)\n\u001b[1;32m    617\u001b[0m     ]\n\u001b[1;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m jtu\u001b[39m.\u001b[39mtree_map(_sum, \u001b[39m*\u001b[39mout)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_term.py:665\u001b[0m, in \u001b[0;36mWrapTerm.prod\u001b[0;34m(self, vf, control)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprod\u001b[39m(\u001b[39mself\u001b[39m, vf: _VF, control: _Control) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Y:\n\u001b[1;32m    664\u001b[0m     \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnumpy_dtype_promotion(\u001b[39m\"\u001b[39m\u001b[39mstandard\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 665\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mterm\u001b[39m.\u001b[39;49mprod(vf, control)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_term.py:479\u001b[0m, in \u001b[0;36mControlTerm.prod\u001b[0;34m(self, vf, control)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39mreturn\u001b[39;00m vf\u001b[39m.\u001b[39mmv(control)\n\u001b[1;32m    478\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m jtu\u001b[39m.\u001b[39;49mtree_map(_prod, vf, control)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/diffrax_extensions/diffrax/_term.py:284\u001b[0m, in \u001b[0;36m_prod\u001b[0;34m(vf, control)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_prod\u001b[39m(vf, control):\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mtensordot(jnp\u001b[39m.\u001b[39mconj(vf), control, axes\u001b[39m=\u001b[39mjnp\u001b[39m.\u001b[39;49mndim(control))\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_diffrax/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3211\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3209\u001b[0m     \u001b[39mreturn\u001b[39;00m a\u001b[39m.\u001b[39mndim\n\u001b[1;32m   3210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 3211\u001b[0m     \u001b[39mreturn\u001b[39;00m asarray(a)\u001b[39m.\u001b[39mndim\n",
      "File \u001b[0;32m~/miniforge3/envs/dev_diffrax/lib/python3.10/site-packages/jax/_src/core.py:714\u001b[0m, in \u001b[0;36mTracer.__array__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m--> 714\u001b[0m   \u001b[39mraise\u001b[39;00m TracerArrayConversionError(\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m: The numpy.ndarray conversion method __array__() was called on traced array with shape float32[3]\nThe error occurred while tracing the function _fn at /Users/owenlockwood/miniforge3/envs/dev_diffrax/lib/python3.10/site-packages/equinox/_eval_shape.py:31 for jit. This concrete value was not available in Python because it depends on the values of the arguments _dynamic[1][0].tprev and _dynamic[1][0].tnext.\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd29b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('dev_diffrax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "01761703e8e304055600d311574f89f8a646f73edac04b8bff1580ad2d98581f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
