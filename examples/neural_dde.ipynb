{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural DDE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use Diffrax in order to solve a Delay Differential Equation (DDE) with known delays.  \n",
    "Unlike ODEs that are identified by their vector field $f(t, y(t))$ and initial condition $y(0)=y_0$, DDEs are specified by their vector field $f$, deviated arguments $y(t-\\tau)$ and history function $\\phi(t)=y(t<0)$.\n",
    "\n",
    "We will model the [Lotka Volterra](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations) (LK) equations with one constant time delay defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& y_1'(t) = \\frac{1}{2} y_1(t) ( 1  - y_2(t-0.2)) \\\\\n",
    "& y_2'(t) = -\\frac{1}{2} y_2(t)( 1  - y_1(t-0.2)) \\\\\n",
    "& \\phi(t) = y(t<0) = (y_1, y_2) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $x_0, y_0$ are uniformly sampled in $[0.1,2]$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is available as a Jupyter notebook [here](url)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to model our problem as a DDE $y'(t) = f_{\\theta}(t, y(t), y(t-\\tau_1), \\dots, y(t-\\tau_d))$, we first need to define a `Delays` object that incorporates deviated arguments in our vector field $f$.  \n",
    "\n",
    "LK's initial time point $t=0$ has a derivative jump because $\\phi^{\\prime}(t=0^{-}) \\neq  y^{\\prime}(t=0^{+})$ and the history function $\\phi(t)$ has `None`.  \n",
    "The DDE model only has one time delay so $d=1$ and our vector field will be $y'(t) = f_{\\theta}(t, y(t), y(t-\\tau))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = diffrax.Delays(\n",
    "    delays=[lambda t, y, args: 0.2], initial_discontinuities=jnp.array([0.0])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is defined the vector field $f_{\\theta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=data_size,\n",
    "            out_size=data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.relu,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args, history):\n",
    "        return self.mlp(y, history[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `history` variable inside the network's `__call__`  is a tuple of deviated arguments. For example, if we possess a `Delays` object with 2 delays then the first element of tuple would be the first deviated argument $y(t-\\tau_1)$ and the second one $y(t-\\tau_1)$.  \n",
    "In our case, `history[0]` corresponds to $y(t-0.2)$ and by extension `history[0][0]` is $y_1(t-0.2)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wrap up the entire DDE solve into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDDE(eqx.Module):\n",
    "    func: Func\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.func = Func(data_size, width_size, depth, key=key)\n",
    "\n",
    "    def __call__(self, ts, y0):\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Tsit5(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=ts[1] - ts[0],\n",
    "            y0=y0,\n",
    "            delays=delays,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=diffrax.SaveAt(ts=ts, dense=True),\n",
    "        )\n",
    "        return solution.ys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the LK dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(ts, *, key):\n",
    "    y0 = jrandom.uniform(key, (2,), minval=0.1, maxval=2.0)\n",
    "\n",
    "    def vector_field(t, y, args, history):\n",
    "        return jnp.array(\n",
    "            [\n",
    "                1 / 2 * y[0] * (1 - history[0][1]),\n",
    "                -1 / 2 * y[1] * (1 - history[0][0]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        diffrax.ODETerm(vector_field),\n",
    "        diffrax.Dopri5(),\n",
    "        t0=ts[0],\n",
    "        t1=ts[-1],\n",
    "        dt0=ts[1] - ts[0],\n",
    "        y0=lambda t: y0,\n",
    "        adjoint=diffrax.NoAdjoint(),\n",
    "        stepsize_controller=diffrax.PIDController(rtol=1e-6, atol=1e-9),\n",
    "        saveat=diffrax.SaveAt(ts=ts, dense=True),\n",
    "        delays=delays,\n",
    "    )\n",
    "\n",
    "    return sol.ys\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.linspace(0, 15, 200)\n",
    "    key = jrandom.split(key, dataset_size)\n",
    "    ys = jax.vmap(lambda key: _get_data(ts, key=key))(key)\n",
    "    return ts, ys\n",
    "\n",
    "\n",
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main entry point. Try runnning `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=256,\n",
    "    batch_size=32,\n",
    "    width_size=32,\n",
    "    depth=2,\n",
    "    tot_steps=500,\n",
    "    lr=10e-3,\n",
    "    seed=5678,\n",
    "    plot=True,\n",
    "    print_every=100,\n",
    "):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    data_key, model_key, loader_key = jrandom.split(key, 3)\n",
    "\n",
    "    ts, ys = get_data(dataset_size, key=data_key)\n",
    "    _, _, data_size = ys.shape\n",
    "\n",
    "    model = NeuralDDE(data_size, width_size, depth, key=model_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def grad_loss(model, ti, yi):\n",
    "        y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n",
    "        return jnp.mean((yi - y_pred) ** 2)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(ti, yi, model, opt_state):\n",
    "        loss, grads = grad_loss(model, ti, yi)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    optim = optax.adabelief(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    for step, (yi,) in zip(\n",
    "        range(tot_steps), dataloader((ys,), batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        loss, model, opt_state = make_step(ts, yi, model, opt_state)\n",
    "        end = time.time()\n",
    "        if (step % print_every) == 0 or step == tot_steps - 1:\n",
    "            print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(ts, ys[0, :, 0], c=\"dodgerblue\", label=\"Real\")\n",
    "        plt.plot(ts, ys[0, :, 1], c=\"dodgerblue\")\n",
    "        model_y = model(ts, ys[0, 0])\n",
    "        plt.plot(ts, model_y[:, 0], c=\"crimson\", label=\"Model\")\n",
    "        plt.plot(ts, model_y[:, 1], c=\"crimson\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"neural_ode.png\")\n",
    "        plt.show()\n",
    "\n",
    "    return ts, ys, model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dde')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85a82c4ca03817695851f28a9a8c882825d82c078a665316024297a0baa050af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
