{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural DDE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use Diffrax in order to solve a Delay Differential Equation (DDE) with known delays.  \n",
    "Unlike ODEs that are identified by their vector field $f(t, y(t))$ and initial condition $y(0)=y_0$, DDEs are specified by their vector field $f$, deviated arguments $y(t-\\tau)$ and history function $\\phi(t)=y(t<0)$.\n",
    "\n",
    "We will model the [Lotka Volterra](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations) (LK) equations with one constant time delay defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& y_1'(t) = \\frac{1}{2} y_1(t) ( 1  - y_2(t-0.2)) \\\\\n",
    "& y_2'(t) = -\\frac{1}{2} y_2(t)( 1  - y_1(t-0.2)) \\\\\n",
    "& \\phi(t) = y(t<0) = (y_{1,0}, y_{2,0}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $y_{1,0}, y_{2,0}$ are uniformly sampled in $[1.0,1.5]$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is available as a Jupyter notebook [here](url)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to model our problem as a DDE $y'(t) = f_{\\theta}(t, y(t), y(t-\\tau_1), \\dots, y(t-\\tau_d))$, we first need to define a `Delays` object that incorporates deviated arguments in our vector field $f$.  \n",
    "\n",
    "LK's initial time point $t=0$ has a derivative jump because $\\phi^{\\prime}(t=0^{-}) \\neq  y^{\\prime}(t=0^{+})$ and the history function $\\phi(t)$ has `None`.  \n",
    "The DDE model only has one time delay so $d=1$ and our vector field will be $y'(t) = f_{\\theta}(t, y(t), y(t-\\tau))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = diffrax.Delays(\n",
    "    delays=[lambda t, y, args: 0.2], initial_discontinuities=jnp.array([0.0])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is defined the vector field $f_{\\theta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=2 * data_size,\n",
    "            out_size=data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.relu,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args, *, history):\n",
    "        return self.mlp(jnp.hstack([y, *history]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `history` variable inside the network's `__call__`  is a tuple of deviated arguments. For example, if we possess a `Delays` object with 2 delays then the first element of tuple would be the first deviated argument $y(t-\\tau_1)$ and the second one $y(t-\\tau_1)$.  \n",
    "In our case, `history[0]` corresponds to $y(t-0.2)$ and by extension `history[0][0]` is $y_1(t-0.2)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wrap up the entire DDE solve into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDDE(eqx.Module):\n",
    "    func: Func\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.func = Func(data_size, width_size, depth, key=key)\n",
    "\n",
    "    def __call__(self, ts, y0):\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Dopri5(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=ts[1] - ts[0],\n",
    "            y0=lambda t: y0,\n",
    "            delays=delays,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=diffrax.SaveAt(ts=ts, dense=True),\n",
    "        )\n",
    "        return solution.ys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the LK dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(ts, *, key):\n",
    "    y0 = jrandom.uniform(key, (2,), minval=1.0, maxval=1.5)\n",
    "\n",
    "    def vector_field(t, y, args, history):\n",
    "        return jnp.array(\n",
    "            [\n",
    "                1 / 2 * y[0] * (1 - history[0][1]),\n",
    "                -1 / 2 * y[1] * (1 - history[0][0]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        diffrax.ODETerm(vector_field),\n",
    "        diffrax.Dopri5(),\n",
    "        t0=ts[0],\n",
    "        t1=ts[-1],\n",
    "        dt0=ts[1] - ts[0],\n",
    "        y0=lambda t: y0,\n",
    "        stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "        saveat=diffrax.SaveAt(ts=ts, dense=True),\n",
    "        delays=delays,\n",
    "    )\n",
    "\n",
    "    return sol.ys\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.linspace(0, 15, 150)\n",
    "    key = jrandom.split(key, dataset_size)\n",
    "    ys = jax.vmap(lambda key: _get_data(ts, key=key))(key)\n",
    "    return ts, ys\n",
    "\n",
    "\n",
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main entry point. Try runnning `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=256,\n",
    "    batch_size=128,\n",
    "    lr_strategy=(3e-3,),\n",
    "    steps_strategy=(500,),\n",
    "    length_strategy=(1.0,),\n",
    "    width_size=32,\n",
    "    depth=3,\n",
    "    seed=5679,\n",
    "    plot=True,\n",
    "    print_every=5,\n",
    "):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    data_key, model_key, loader_key = jrandom.split(key, 3)\n",
    "\n",
    "    ts, ys = get_data(dataset_size, key=data_key)\n",
    "    mean, std = jnp.mean(ys), jnp.std(ys)\n",
    "    ys = (ys - mean) / std\n",
    "    _, length_size, data_size = ys.shape\n",
    "\n",
    "    model = NeuralDDE(data_size, width_size, depth, key=model_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def grad_loss(model, ti, yi):\n",
    "        y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n",
    "        return jnp.mean((yi - y_pred) ** 2)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(ti, yi, model, opt_state):\n",
    "        loss, grads = grad_loss(model, ti, yi)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n",
    "        optim = optax.adabelief(lr)\n",
    "        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "        _ts = ts[: int(length_size * length)]\n",
    "        _ys = ys[:, : int(length_size * length)]\n",
    "        for step, (yi,) in zip(\n",
    "            range(steps), dataloader((_ys,), batch_size, key=loader_key)\n",
    "        ):\n",
    "            start = time.time()\n",
    "            loss, model, opt_state = make_step(_ts, yi, model, opt_state)\n",
    "            end = time.time()\n",
    "            if (step % print_every) == 0 or step == steps - 1:\n",
    "                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(_ts, _ys[0, :, 0], c=\"dodgerblue\", label=\"Real\")\n",
    "        plt.plot(_ts, _ys[0, :, 1], c=\"dodgerblue\")\n",
    "        model_y = model(_ts, _ys[0, 0])\n",
    "        plt.plot(_ts, model_y[:, 0], c=\"crimson\", label=\"Model\")\n",
    "        plt.plot(_ts, model_y[:, 1], c=\"crimson\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"neural_dde.png\")\n",
    "        plt.close()\n",
    "\n",
    "    return ts, ys, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.401884913444519, Computation time: 14.241726636886597\n",
      "Step: 5, Loss: 1.134800910949707, Computation time: 1.192063570022583\n",
      "Step: 10, Loss: 0.9904348254203796, Computation time: 1.9642481803894043\n",
      "Step: 15, Loss: 0.9832042455673218, Computation time: 2.6581308841705322\n",
      "Step: 20, Loss: 0.9569293260574341, Computation time: 3.8219752311706543\n",
      "Step: 25, Loss: 0.9038560390472412, Computation time: 3.6977927684783936\n",
      "Step: 30, Loss: 0.8163420557975769, Computation time: 35.42212176322937\n",
      "Step: 35, Loss: 0.6886805295944214, Computation time: 6.909891843795776\n",
      "Step: 40, Loss: 0.3398251533508301, Computation time: 5.504199266433716\n",
      "Step: 45, Loss: 0.16126586496829987, Computation time: 5.103270769119263\n",
      "Step: 50, Loss: 0.08195476979017258, Computation time: 5.78333044052124\n",
      "Step: 55, Loss: 0.06952822208404541, Computation time: 10.413585901260376\n",
      "Step: 60, Loss: 0.031119057908654213, Computation time: 7.735013723373413\n",
      "Step: 65, Loss: 0.025405289605259895, Computation time: 7.267561912536621\n",
      "Step: 70, Loss: 0.017744384706020355, Computation time: 6.29371190071106\n",
      "Step: 75, Loss: 0.01782667636871338, Computation time: 25.47852373123169\n",
      "Step: 80, Loss: 0.015500097535550594, Computation time: 6.827113628387451\n",
      "Step: 85, Loss: 0.011661469005048275, Computation time: 7.4889373779296875\n",
      "Step: 90, Loss: 0.00916498713195324, Computation time: 5.9714484214782715\n",
      "Step: 95, Loss: 0.010490368120372295, Computation time: 6.338549613952637\n",
      "Step: 100, Loss: 0.007394495420157909, Computation time: 6.486926078796387\n",
      "Step: 105, Loss: 0.007423707749694586, Computation time: 7.48775839805603\n",
      "Step: 110, Loss: 0.007470142096281052, Computation time: 6.819472312927246\n",
      "Step: 115, Loss: 0.0059195710346102715, Computation time: 6.100851058959961\n",
      "Step: 120, Loss: 0.005787399597465992, Computation time: 6.755363941192627\n",
      "Step: 125, Loss: 0.005841915961354971, Computation time: 6.580211639404297\n",
      "Step: 130, Loss: 0.006159897893667221, Computation time: 6.810395956039429\n",
      "Step: 135, Loss: 0.0052039725705981255, Computation time: 7.518130779266357\n",
      "Step: 140, Loss: 0.0053937858901917934, Computation time: 6.242229223251343\n",
      "Step: 145, Loss: 0.00430111913010478, Computation time: 5.085596799850464\n",
      "Step: 150, Loss: 0.004397619515657425, Computation time: 5.2558135986328125\n",
      "Step: 155, Loss: 0.004206538666039705, Computation time: 7.212834596633911\n"
     ]
    }
   ],
   "source": [
    "ts, ys, model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_diffrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
