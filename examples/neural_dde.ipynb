{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural DDE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates how to use Diffrax in order to solve a Delay Differential Equation (DDE) with known delays.  \n",
    "Unlike ODEs that are identified by their vector field $f(t, y(t))$ and initial condition $y(0)=y_0$, DDEs are specified by their vector field $f$, deviated arguments $y(t-\\tau)$ and history function $\\phi(t)=y(t<0)$.\n",
    "\n",
    "We will model the [Lotka Volterra](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations) (LK) equations with one constant time delay defined as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& y_1'(t) = \\frac{1}{2} y_1(t) ( 1  - y_2(t-0.2)) \\\\\n",
    "& y_2'(t) = -\\frac{1}{2} y_2(t)( 1  - y_1(t-0.2)) \\\\\n",
    "& \\phi(t) = y(t<0) = (y_{1,0}, y_{2,0}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $y_{1,0}, y_{2,0}$ are uniformly sampled in $[1.0,1.5]$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is available as a Jupyter notebook [here](url)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to model our problem as a DDE $y'(t) = f_{\\theta}(t, y(t), y(t-\\tau_1), \\dots, y(t-\\tau_d))$, we first need to define a `Delays` object that incorporates deviated arguments in our vector field $f$.  \n",
    "\n",
    "LK's initial time point $t=0$ has a derivative jump because $\\phi^{\\prime}(t=0^{-}) \\neq  y^{\\prime}(t=0^{+})$ and the history function $\\phi(t)$ has `None`.  \n",
    "The DDE model only has one time delay so $d=1$ and our vector field will be $y'(t) = f_{\\theta}(t, y(t), y(t-\\tau))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays = diffrax.Delays(\n",
    "    delays=[lambda t, y, args: 0.2], initial_discontinuities=jnp.array([0.0])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is defined the vector field $f_{\\theta}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=2 * data_size,\n",
    "            out_size=data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.relu,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args, *, history):\n",
    "        return self.mlp(jnp.hstack([y, *history]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `history` variable inside the network's `__call__`  is a tuple of deviated arguments. For example, if we possess a `Delays` object with 2 delays then the first element of tuple would be the first deviated argument $y(t-\\tau_1)$ and the second one $y(t-\\tau_1)$.  \n",
    "In our case, `history[0]` corresponds to $y(t-0.2)$ and by extension `history[0][0]` is $y_1(t-0.2)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we wrap up the entire DDE solve into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDDE(eqx.Module):\n",
    "    func: Func\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.func = Func(data_size, width_size, depth, key=key)\n",
    "\n",
    "    def __call__(self, ts, y0):\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Dopri5(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=ts[1] - ts[0],\n",
    "            y0=lambda t: y0,\n",
    "            delays=delays,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=diffrax.SaveAt(ts=ts, dense=True),\n",
    "        )\n",
    "        return solution.ys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the LK dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(ts, *, key):\n",
    "    y0 = jrandom.uniform(key, (2,), minval=1.0, maxval=1.5)\n",
    "\n",
    "    def vector_field(t, y, args, history):\n",
    "        return jnp.array(\n",
    "            [\n",
    "                1 / 2 * y[0] * (1 - history[0][1]),\n",
    "                -1 / 2 * y[1] * (1 - history[0][0]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        diffrax.ODETerm(vector_field),\n",
    "        diffrax.Dopri5(),\n",
    "        t0=ts[0],\n",
    "        t1=ts[-1],\n",
    "        dt0=ts[1] - ts[0],\n",
    "        y0=lambda t: y0,\n",
    "        stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "        saveat=diffrax.SaveAt(ts=ts, dense=True),\n",
    "        delays=delays,\n",
    "    )\n",
    "\n",
    "    return sol.ys\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.linspace(0, 15, 150)\n",
    "    key = jrandom.split(key, dataset_size)\n",
    "    ys = jax.vmap(lambda key: _get_data(ts, key=key))(key)\n",
    "    return ts, ys\n",
    "\n",
    "\n",
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main entry point. Try runnning `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    dataset_size=256,\n",
    "    batch_size=128,\n",
    "    lr_strategy=(3e-3,),\n",
    "    steps_strategy=(120,),\n",
    "    length_strategy=(1.0,),\n",
    "    width_size=32,\n",
    "    depth=3,\n",
    "    seed=5679,\n",
    "    plot=True,\n",
    "    print_every=5,\n",
    "):\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    data_key, model_key, loader_key = jrandom.split(key, 3)\n",
    "\n",
    "    ts, ys = get_data(dataset_size, key=data_key)\n",
    "    mean, std = jnp.mean(ys), jnp.std(ys)\n",
    "    ys = (ys - mean) / std\n",
    "    _, length_size, data_size = ys.shape\n",
    "\n",
    "    model = NeuralDDE(data_size, width_size, depth, key=model_key)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def grad_loss(model, ti, yi):\n",
    "        y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n",
    "        return jnp.mean((yi - y_pred) ** 2)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(ti, yi, model, opt_state):\n",
    "        loss, grads = grad_loss(model, ti, yi)\n",
    "        updates, opt_state = optim.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n",
    "\n",
    "    for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n",
    "        optim = optax.adabelief(lr)\n",
    "        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "        _ts = ts[: int(length_size * length)]\n",
    "        _ys = ys[:, : int(length_size * length)]\n",
    "        for step, (yi,) in zip(\n",
    "            range(steps), dataloader((_ys,), batch_size, key=loader_key)\n",
    "        ):\n",
    "            start = time.time()\n",
    "            loss, model, opt_state = make_step(_ts, yi, model, opt_state)\n",
    "            end = time.time()\n",
    "            if (step % print_every) == 0 or step == steps - 1:\n",
    "                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(_ts, _ys[0, :, 0], c=\"dodgerblue\", label=\"Real\")\n",
    "        plt.plot(_ts, _ys[0, :, 1], c=\"dodgerblue\")\n",
    "        model_y = model(_ts, _ys[0, 0])\n",
    "        plt.plot(_ts, model_y[:, 0], c=\"crimson\", label=\"Model\")\n",
    "        plt.plot(_ts, model_y[:, 1], c=\"crimson\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"neural_dde.png\")\n",
    "        plt.close()\n",
    "\n",
    "    return ts, ys, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 1.401884913444519, Computation time: 14.423743486404419\n",
      "Step: 1, Loss: 1.1716960668563843, Computation time: 0.00439143180847168\n",
      "Step: 2, Loss: 1.244404673576355, Computation time: 0.003352642059326172\n",
      "Step: 3, Loss: 1.152793049812317, Computation time: 0.0034520626068115234\n",
      "Step: 4, Loss: 1.2006347179412842, Computation time: 0.0023109912872314453\n",
      "Step: 5, Loss: 1.134800910949707, Computation time: 0.003433704376220703\n",
      "Step: 6, Loss: 1.0922808647155762, Computation time: 0.00430607795715332\n",
      "Step: 7, Loss: 1.0771468877792358, Computation time: 0.0031037330627441406\n",
      "Step: 8, Loss: 1.1282471418380737, Computation time: 0.0026216506958007812\n",
      "Step: 9, Loss: 1.0664700269699097, Computation time: 0.0036363601684570312\n",
      "Step: 10, Loss: 0.9904348254203796, Computation time: 0.0024759769439697266\n",
      "Step: 11, Loss: 1.0465335845947266, Computation time: 0.002873659133911133\n",
      "Step: 12, Loss: 1.0017845630645752, Computation time: 0.004104137420654297\n",
      "Step: 13, Loss: 1.0248370170593262, Computation time: 0.002857208251953125\n",
      "Step: 14, Loss: 0.9131743311882019, Computation time: 0.0031785964965820312\n",
      "Step: 15, Loss: 0.9832042455673218, Computation time: 0.003298044204711914\n",
      "Step: 16, Loss: 0.9668886661529541, Computation time: 0.002664327621459961\n",
      "Step: 17, Loss: 0.9930294752120972, Computation time: 0.0028023719787597656\n",
      "Step: 18, Loss: 0.9732811450958252, Computation time: 0.004416465759277344\n",
      "Step: 19, Loss: 0.9101904630661011, Computation time: 0.0021491050720214844\n",
      "Step: 20, Loss: 0.9569293260574341, Computation time: 0.003906965255737305\n",
      "Step: 21, Loss: 1.000235676765442, Computation time: 0.0031125545501708984\n",
      "Step: 22, Loss: 0.948157787322998, Computation time: 0.0025708675384521484\n",
      "Step: 23, Loss: 0.9466567039489746, Computation time: 0.003038644790649414\n",
      "Step: 24, Loss: 0.9593256115913391, Computation time: 0.003378629684448242\n",
      "Step: 25, Loss: 0.9038560390472412, Computation time: 0.003355264663696289\n",
      "Step: 26, Loss: 0.9106528162956238, Computation time: 0.0029053688049316406\n",
      "Step: 27, Loss: 0.8822335004806519, Computation time: 0.0023543834686279297\n",
      "Step: 28, Loss: 0.8793952465057373, Computation time: 0.002279043197631836\n",
      "Step: 29, Loss: 0.8758573532104492, Computation time: 0.0025527477264404297\n",
      "Step: 30, Loss: 0.8163420557975769, Computation time: 0.0034754276275634766\n",
      "Step: 31, Loss: 0.7726603150367737, Computation time: 0.0041882991790771484\n",
      "Step: 32, Loss: 0.7940323352813721, Computation time: 0.0024156570434570312\n",
      "Step: 33, Loss: 0.7175382375717163, Computation time: 0.0027916431427001953\n",
      "Step: 34, Loss: 0.7028713226318359, Computation time: 0.0023674964904785156\n",
      "Step: 35, Loss: 0.6886805295944214, Computation time: 0.0023441314697265625\n",
      "Step: 36, Loss: 0.6005609035491943, Computation time: 0.002215862274169922\n",
      "Step: 37, Loss: 0.5269209742546082, Computation time: 0.003516674041748047\n",
      "Step: 38, Loss: 0.4020206332206726, Computation time: 0.0025489330291748047\n",
      "Step: 39, Loss: 0.3255264461040497, Computation time: 0.0038254261016845703\n",
      "Step: 40, Loss: 0.3398251533508301, Computation time: 0.0026879310607910156\n",
      "Step: 41, Loss: 0.23914429545402527, Computation time: 0.0031821727752685547\n",
      "Step: 42, Loss: 0.14592041075229645, Computation time: 0.002313852310180664\n",
      "Step: 43, Loss: 0.13987970352172852, Computation time: 0.002420186996459961\n",
      "Step: 44, Loss: 0.1373867690563202, Computation time: 0.002546548843383789\n",
      "Step: 45, Loss: 0.16126586496829987, Computation time: 0.0025119781494140625\n",
      "Step: 46, Loss: 0.11544477194547653, Computation time: 0.0023653507232666016\n",
      "Step: 47, Loss: 0.061478693038225174, Computation time: 0.0027551651000976562\n",
      "Step: 48, Loss: 0.042316604405641556, Computation time: 0.0026128292083740234\n",
      "Step: 49, Loss: 0.09910032153129578, Computation time: 0.0021860599517822266\n",
      "Step: 50, Loss: 0.08195476979017258, Computation time: 0.0030493736267089844\n",
      "Step: 51, Loss: 0.036347728222608566, Computation time: 0.0036330223083496094\n",
      "Step: 52, Loss: 0.04329509660601616, Computation time: 0.002357959747314453\n",
      "Step: 53, Loss: 0.0774608924984932, Computation time: 0.003058910369873047\n",
      "Step: 54, Loss: 0.07515737414360046, Computation time: 0.0029230117797851562\n",
      "Step: 55, Loss: 0.06952822208404541, Computation time: 0.004669904708862305\n",
      "Step: 56, Loss: 0.04167735204100609, Computation time: 0.004377126693725586\n",
      "Step: 57, Loss: 0.043842863291502, Computation time: 0.002735614776611328\n",
      "Step: 58, Loss: 0.06545793265104294, Computation time: 0.003109455108642578\n",
      "Step: 59, Loss: 0.055285390466451645, Computation time: 0.003002643585205078\n",
      "Step: 60, Loss: 0.031119057908654213, Computation time: 0.0036330223083496094\n",
      "Step: 61, Loss: 0.03198694810271263, Computation time: 0.0037326812744140625\n",
      "Step: 62, Loss: 0.039938874542713165, Computation time: 0.003992319107055664\n",
      "Step: 63, Loss: 0.045956145972013474, Computation time: 0.0034766197204589844\n",
      "Step: 64, Loss: 0.03436319902539253, Computation time: 0.0034666061401367188\n",
      "Step: 65, Loss: 0.025405289605259895, Computation time: 0.003632783889770508\n",
      "Step: 66, Loss: 0.023711830377578735, Computation time: 0.002114534378051758\n",
      "Step: 67, Loss: 0.03284265473484993, Computation time: 0.0043947696685791016\n",
      "Step: 68, Loss: 0.03023228421807289, Computation time: 0.003482341766357422\n",
      "Step: 69, Loss: 0.021171605214476585, Computation time: 0.0029871463775634766\n",
      "Step: 70, Loss: 0.017744384706020355, Computation time: 0.003162860870361328\n",
      "Step: 71, Loss: 0.022380519658327103, Computation time: 0.004060029983520508\n",
      "Step: 72, Loss: 0.02576189674437046, Computation time: 0.0023908615112304688\n",
      "Step: 73, Loss: 0.019239962100982666, Computation time: 0.002658367156982422\n",
      "Step: 74, Loss: 0.016447639092803, Computation time: 0.003663301467895508\n",
      "Step: 75, Loss: 0.01782667636871338, Computation time: 0.0035588741302490234\n",
      "Step: 76, Loss: 0.02072978764772415, Computation time: 0.0031981468200683594\n"
     ]
    }
   ],
   "source": [
    "ts, ys, model = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_diffrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
