{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc5e6de-fd8f-4589-a5bc-8169cdc12ca1",
   "metadata": {},
   "source": [
    "# Neural SDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d198be-5b0f-4d5d-a8d1-a63a3b24ed9d",
   "metadata": {},
   "source": [
    "This example constructs a neural SDE as a generative time series model.\n",
    "\n",
    "An SDE is, of course, random: it defines some distribution. Each sample is a whole path. Thus in modern machine learning parlance, an SDE is a generative time series model. This means it can be trained as a GAN, for example. This does mean we need a discriminator that consumes a path as an input; we use a CDE.\n",
    "\n",
    "Training an SDE as a GAN is precisely what this example does. Doing so will reproduce the following toy example:\n",
    "\n",
    "![ou](../imgs/neural_sde.png)\n",
    "\n",
    "**References:**\n",
    "\n",
    "Training SDEs as GANs:\n",
    "```bibtex\n",
    "@inproceedings{kidger2021sde1,\n",
    "    title={{N}eural {SDE}s as {I}nfinite-{D}imensional {GAN}s},\n",
    "    author={Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry J},\n",
    "    booktitle = {Proceedings of the 38th International Conference on Machine Learning},\n",
    "    pages = {5453--5463},\n",
    "    year = {2021},\n",
    "    volume = {139},\n",
    "    series = {Proceedings of Machine Learning Research},\n",
    "    publisher = {PMLR},\n",
    "}\n",
    "```\n",
    "\n",
    "Improved training techniques:\n",
    "```bibtex\n",
    "@incollection{kidger2021sde2,\n",
    "    title={{E}fficient and {A}ccurate {G}radients for {N}eural {SDE}s},\n",
    "    author={Kidger, Patrick and Foster, James and Li, Xuechen and Lyons, Terry},\n",
    "    booktitle = {Advances in Neural Information Processing Systems 34},\n",
    "    year = {2021},\n",
    "    publisher = {Curran Associates, Inc.},\n",
    "}\n",
    "```\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ecd31-c6f3-4cff-adbc-2f880c40f11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax\n",
    "import tqdm  # https://github.com/tqdm/tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88968c8a-7c63-4557-a332-75565b7c8721",
   "metadata": {},
   "source": [
    "LipSwish activation functions are a good choice for the discriminator of an SDE-GAN. (Their use here was introduced in the second reference above.)\n",
    "For simplicity we will actually use LipSwish activations everywhere, even in the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41f97b-8b00-49c4-84fe-b35f340b7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lipswish(x):\n",
    "    return 0.909 * jnn.silu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415f14e-9195-4179-9430-0983faecab39",
   "metadata": {},
   "source": [
    "Now set up the vector fields appearing on the right hand side of each differential equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dad43-7a89-4485-8b74-7855931d2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorField(eqx.Module):\n",
    "    scale: Union[int, jnp.ndarray]\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, hidden_size, width_size, depth, scale, *, key):\n",
    "        super().__init__()\n",
    "        scale_key, mlp_key = jrandom.split(key)\n",
    "        if scale:\n",
    "            self.scale = jrandom.uniform(\n",
    "                scale_key, (hidden_size,), minval=0.9, maxval=1.1\n",
    "            )\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y]))\n",
    "\n",
    "\n",
    "class ControlledVectorField(eqx.Module):\n",
    "    scale: Union[int, jnp.ndarray]\n",
    "    mlp: eqx.nn.MLP\n",
    "    control_size: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def __init__(self, control_size, hidden_size, width_size, depth, scale, *, key):\n",
    "        super().__init__()\n",
    "        scale_key, mlp_key = jrandom.split(key)\n",
    "        if scale:\n",
    "            self.scale = jrandom.uniform(\n",
    "                scale_key, (hidden_size, control_size), minval=0.9, maxval=1.1\n",
    "            )\n",
    "        else:\n",
    "            self.scale = 1\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=hidden_size + 1,\n",
    "            out_size=hidden_size * control_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=lipswish,\n",
    "            final_activation=jnn.tanh,\n",
    "            key=mlp_key,\n",
    "        )\n",
    "        self.control_size = control_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.scale * self.mlp(jnp.concatenate([t[None], y])).reshape(\n",
    "            self.hidden_size, self.control_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd0e21-4764-427a-af80-fbe4cebcb6ac",
   "metadata": {},
   "source": [
    "Something pretty neat: for this toy example, both the generator and the discriminator have essentially the same structure. We describe this through this `DifferentialEquation` class that we'll use for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0ff69-95b3-4653-b773-24fa77ca5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialEquation(eqx.Module):\n",
    "    initial: eqx.nn.MLP\n",
    "    vf: VectorField  # drift\n",
    "    cvf: ControlledVectorField  # diffusion\n",
    "    readout: eqx.nn.Linear\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        initial_size,\n",
    "        control_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        readout_size,\n",
    "        scale,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        initial_key, vf_key, cvf_key, readout_key = jrandom.split(key, 4)\n",
    "\n",
    "        self.initial = eqx.nn.MLP(\n",
    "            initial_size, hidden_size, width_size, depth, key=initial_key\n",
    "        )\n",
    "        self.vf = VectorField(hidden_size, width_size, depth, scale, key=vf_key)\n",
    "        self.cvf = ControlledVectorField(\n",
    "            control_size, hidden_size, width_size, depth, scale, key=cvf_key\n",
    "        )\n",
    "        self.readout = eqx.nn.Linear(hidden_size, readout_size, key=readout_key)\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def _initial(self, ts, init):\n",
    "        t0 = ts[0]\n",
    "        t1 = ts[-1]\n",
    "        y0 = self.initial(init)\n",
    "        # We use a huge step size. By using a large step size we essentially \"bake in\"\n",
    "        # the discretisation.\n",
    "        # This is quite a standard thing to do when the vector field is a pure neural\n",
    "        # network.\n",
    "        # You can reduce the step size here if you want to -- which will increase the\n",
    "        # computational cost, of course.\n",
    "        dt0 = 1.0\n",
    "        return t0, t1, y0, dt0\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def _readout(self, ys):\n",
    "        return jax.vmap(self.readout)(ys)\n",
    "\n",
    "    def __call__(self, ts, init, control, saveat):\n",
    "        vf = diffrax.ODETerm(self.vf)  # Drift term\n",
    "        cvf = diffrax.ControlTerm(self.cvf, control)  # Diffusion term\n",
    "        term = diffrax.MultiTerm((vf, cvf))  # Combine both terms\n",
    "        solver = diffrax.ReversibleHeun(term)  # Choice of solver.\n",
    "\n",
    "        t0, t1, y0, dt0 = self._initial(ts, init)\n",
    "        sol = diffrax.diffeqsolve(solver, t0, t1, y0, dt0, saveat=saveat)\n",
    "        return self._readout(sol.ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4633fb9-97af-4e22-b903-40a1fc6c92ad",
   "metadata": {},
   "source": [
    "Now set up the neural SDE (the generator) and the neural CDE (the discriminator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c157fe-4c86-4e15-9020-b523b517ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSDE(eqx.Module):\n",
    "    diffeq: DifferentialEquation\n",
    "    initial_noise_size: int\n",
    "    noise_size: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        *,\n",
    "        key,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.diffeq = DifferentialEquation(\n",
    "            initial_size=initial_noise_size,\n",
    "            control_size=noise_size,\n",
    "            hidden_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            readout_size=data_size,\n",
    "            scale=True,  # Needed for a flexible generator.\n",
    "            key=key,\n",
    "        )\n",
    "        self.initial_noise_size = initial_noise_size\n",
    "        self.noise_size = noise_size\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def _setup(self, ts, key):\n",
    "        init_key, bm_key = jrandom.split(key, 2)\n",
    "        init = jrandom.normal(init_key, (self.initial_noise_size,))\n",
    "        control = diffrax.UnsafeBrownianPath(shape=(self.noise_size,), key=bm_key)\n",
    "        saveat = diffrax.SaveAt(ts=ts)  # Record output at all times.\n",
    "        return ts, init, control, saveat\n",
    "\n",
    "    def __call__(self, ts, *, key):\n",
    "        return self.diffeq(*self._setup(ts, key))\n",
    "\n",
    "\n",
    "class NeuralCDE(eqx.Module):\n",
    "    diffeq: DifferentialEquation\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, width_size, depth, *, key):\n",
    "        super().__init__()\n",
    "        self.diffeq = DifferentialEquation(\n",
    "            initial_size=data_size + 1,\n",
    "            control_size=data_size,\n",
    "            hidden_size=hidden_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            readout_size=1,\n",
    "            # Want to constrain the Lipschitz norm of the discriminator. (See also the\n",
    "            # `clip_weights` method below.)\n",
    "            scale=False,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    @eqx.filter_jit\n",
    "    def _setup(ts, ys):\n",
    "        # Interpolate data into a continuous path.\n",
    "        ys = diffrax.linear_interpolation(\n",
    "            ts, ys, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "        init = jnp.concatenate([ts[0, None], ys[0]])\n",
    "        control = diffrax.LinearInterpolation(ts, ys)\n",
    "        # Have the discriminator produce an output at both `t0` *and* `t1`.\n",
    "        # The output at `t0` has only seen the initial point of a sample. This gives\n",
    "        # additional supervision to the distribution learnt for the initial condition.\n",
    "        # The output at `t1` has seen the entire path of a sample. This is needed to\n",
    "        # actually learn the evolving trajectory.\n",
    "        saveat = diffrax.SaveAt(t0=True, t1=True)\n",
    "        return ts, init, control, saveat\n",
    "\n",
    "    def __call__(self, ts, ys):\n",
    "        return self.diffeq(*self._setup(ts, ys))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def clip_weights(self):\n",
    "        # Equinox modules are just PyTrees like any other, so we can flatten them etc.\n",
    "        # like any other PyTree.\n",
    "        leaves, treedef = jax.tree_flatten(\n",
    "            self, is_leaf=lambda x: isinstance(x, eqx.nn.Linear)\n",
    "        )\n",
    "        new_leaves = []\n",
    "        for leaf in leaves:\n",
    "            if isinstance(leaf, eqx.nn.Linear):\n",
    "                lim = 1 / leaf.out_features\n",
    "                leaf = eqx.tree_at(\n",
    "                    lambda x: x.weight, leaf, leaf.weight.clip(-lim, lim)\n",
    "                )\n",
    "            new_leaves.append(leaf)\n",
    "        return jax.tree_unflatten(treedef, new_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303187f2-95a1-4ed0-b151-3c3f49b9ab21",
   "metadata": {},
   "source": [
    "Note `clip_weights` method on the CDE above -- this is part of imposing the Lipschitz condition on the discriminator of a Wasserstein GAN.\n",
    "(The other thing doing this is the use of those LipSwish activation functions we saw earlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bc8f0-2309-43ea-a23f-7e7f558e9691",
   "metadata": {},
   "source": [
    "Coming up we now have mostly standard stuff for training a GAN: creating the dataset, training, plotting results etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895114b6-44ea-4013-94cf-aa773ce7583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(key):\n",
    "    bm_key, y0_key, drop_key = jrandom.split(key, 3)\n",
    "\n",
    "    mu = 0.02\n",
    "    theta = 0.1\n",
    "    sigma = 0.4\n",
    "\n",
    "    t0 = 0\n",
    "    t1 = 63\n",
    "    t_size = 64\n",
    "\n",
    "    def drift(t, y, args):\n",
    "        return mu * t - theta * y\n",
    "\n",
    "    def diffusion(t, y, args):\n",
    "        return 2 * sigma * t / t1\n",
    "\n",
    "    bm = diffrax.UnsafeBrownianPath(shape=(), key=bm_key)\n",
    "    solver = diffrax.euler_maruyama(drift, diffusion, bm)\n",
    "    y0 = jrandom.uniform(y0_key, (1,), minval=-1, maxval=1)\n",
    "    dt0 = 0.1\n",
    "    ts = jnp.linspace(t0, t1, t_size)\n",
    "    saveat = diffrax.SaveAt(ts=ts)\n",
    "    sol = diffrax.diffeqsolve(solver, t0, t1, y0, dt0, saveat=saveat)\n",
    "    ys = sol.ys\n",
    "\n",
    "    to_drop = jrandom.bernoulli(drop_key, 0.3, (t_size, 1))\n",
    "    ys = jnp.where(to_drop, jnp.nan, ys)\n",
    "\n",
    "    return ts, ys\n",
    "\n",
    "\n",
    "def make_dataloader(arrays, batch_size, loop, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "        if not loop:\n",
    "            break\n",
    "\n",
    "\n",
    "def loss(generator, discriminator, ts_i, ys_i, key, step=0):\n",
    "    batch_size, _ = ts_i.shape\n",
    "    key = jrandom.fold_in(key, step)\n",
    "    key = jrandom.split(key, batch_size)\n",
    "    fake_ys_i = jax.vmap(generator)(ts_i, key=key)\n",
    "    real_score = jax.vmap(discriminator)(ts_i, ys_i)\n",
    "    fake_score = jax.vmap(discriminator)(ts_i, fake_ys_i)\n",
    "    return jnp.mean(real_score - fake_score)\n",
    "\n",
    "\n",
    "@eqx.filter_grad\n",
    "def grad_loss(g_d, ts_i, ys_i, key, step):\n",
    "    generator, discriminator = g_d  # We differentiate just the first argument\n",
    "    return loss(generator, discriminator, ts_i, ys_i, key, step)\n",
    "\n",
    "\n",
    "# This is one very helpful trick. The distribution learnt for the initial condition of\n",
    "# the SDE can sometimes train quite poorly.\n",
    "# Increasing its learning rate (here by a factor of 10) seems to help with this.\n",
    "def _increase_update_initial(updates):\n",
    "    get_initial_leaves = lambda u: jax.tree_leaves(u.diffeq.initial)\n",
    "    mul = lambda x: x * 10\n",
    "    return eqx.tree_at(where=get_initial_leaves, pytree=updates, replace_fn=mul)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def update(\n",
    "    generator, discriminator, g_opt_state, d_opt_state, g_optim, d_optim, g_grad, d_grad\n",
    "):\n",
    "    g_updates, g_opt_state = g_optim.update(g_grad, g_opt_state)\n",
    "    d_updates, d_opt_state = d_optim.update(d_grad, d_opt_state)\n",
    "    g_updates = _increase_update_initial(g_updates)\n",
    "    d_updates = _increase_update_initial(d_updates)\n",
    "    generator = eqx.apply_updates(generator, g_updates)\n",
    "    discriminator = eqx.apply_updates(discriminator, d_updates)\n",
    "    discriminator = discriminator.clip_weights()\n",
    "    return generator, discriminator, g_opt_state, d_opt_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421f671-dcc6-4338-a8f7-9fd17d6aee37",
   "metadata": {},
   "source": [
    "This is our main entry point. Try running `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0581722-97fb-4771-94da-c65f9929e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    initial_noise_size=5,\n",
    "    noise_size=3,\n",
    "    hidden_size=16,\n",
    "    width_size=16,\n",
    "    depth=1,\n",
    "    generator_lr=2e-5,\n",
    "    discriminator_lr=1e-4,\n",
    "    batch_size=1024,\n",
    "    steps=10000,\n",
    "    steps_per_print=10,\n",
    "    dataset_size=8192,\n",
    "    seed=5678,\n",
    "):\n",
    "\n",
    "    key = jrandom.PRNGKey(seed)\n",
    "    (\n",
    "        data_key,\n",
    "        generator_key,\n",
    "        discriminator_key,\n",
    "        dataloader_key,\n",
    "        train_key,\n",
    "        evaluate_key,\n",
    "        sample_key,\n",
    "    ) = jrandom.split(key, 7)\n",
    "    data_key = jrandom.split(data_key, dataset_size)\n",
    "\n",
    "    ts, ys = jax.vmap(get_data)(data_key)\n",
    "    _, _, data_size = ys.shape\n",
    "\n",
    "    generator = NeuralSDE(\n",
    "        data_size,\n",
    "        initial_noise_size,\n",
    "        noise_size,\n",
    "        hidden_size,\n",
    "        width_size,\n",
    "        depth,\n",
    "        key=generator_key,\n",
    "    )\n",
    "    discriminator = NeuralCDE(\n",
    "        data_size, hidden_size, width_size, depth, key=discriminator_key\n",
    "    )\n",
    "\n",
    "    g_optim = optax.rmsprop(generator_lr)\n",
    "    d_optim = optax.rmsprop(-discriminator_lr)\n",
    "    g_opt_state = g_optim.init(eqx.filter(generator, eqx.is_array))\n",
    "    d_opt_state = d_optim.init(eqx.filter(discriminator, eqx.is_array))\n",
    "\n",
    "    trange = tqdm.tqdm(range(steps))\n",
    "    infinite_dataloader = make_dataloader(\n",
    "        (ts, ys), batch_size, loop=True, key=dataloader_key\n",
    "    )\n",
    "\n",
    "    for step, (ts_i, ys_i) in zip(trange, infinite_dataloader):\n",
    "        g_grad, d_grad = grad_loss((generator, discriminator), ts_i, ys_i, key, step)\n",
    "        generator, discriminator, g_opt_state, d_opt_state = update(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            g_opt_state,\n",
    "            d_opt_state,\n",
    "            g_optim,\n",
    "            d_optim,\n",
    "            g_grad,\n",
    "            d_grad,\n",
    "        )\n",
    "\n",
    "        if (step % steps_per_print) == 0 or step == steps - 1:\n",
    "            total_score = 0\n",
    "            num_batches = 0\n",
    "            for ts_i, ys_i in make_dataloader(\n",
    "                (ts, ys), batch_size, loop=False, key=evaluate_key\n",
    "            ):\n",
    "                score = loss(generator, discriminator, ts_i, ys_i, sample_key)\n",
    "                total_score += score.item()\n",
    "                num_batches += 1\n",
    "            trange.write(f\"Step: {step}, Loss: {total_score / num_batches}\")\n",
    "\n",
    "    # Plot samples\n",
    "    fig, ax = plt.subplots()\n",
    "    num_samples = min(50, dataset_size)\n",
    "    ts_to_plot = ts[:num_samples]\n",
    "    ys_to_plot = ys[:num_samples]\n",
    "\n",
    "    def _interp(ti, yi):\n",
    "        return diffrax.linear_interpolation(\n",
    "            ti, yi, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n",
    "        )\n",
    "\n",
    "    ys_to_plot = jax.vmap(_interp)(ts_to_plot, ys_to_plot)[..., 0]\n",
    "    ys_sampled = jax.vmap(generator)(\n",
    "        ts_to_plot, key=jrandom.split(sample_key, num_samples)\n",
    "    )[..., 0]\n",
    "    kwargs = dict(label=\"Real\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_to_plot):\n",
    "        ax.plot(ti, yi, c=\"dodgerblue\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    kwargs = dict(label=\"Generated\")\n",
    "    for ti, yi in zip(ts_to_plot, ys_sampled):\n",
    "        ax.plot(ti, yi, c=\"crimson\", linewidth=0.5, alpha=0.7, **kwargs)\n",
    "        kwargs = {}\n",
    "    ax.set_title(f\"{num_samples} samples from both real and generated distributions.\")\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"neural_sde.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 CUDA 1",
   "language": "python",
   "name": "python3cuda1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
